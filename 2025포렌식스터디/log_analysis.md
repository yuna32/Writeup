AWK 활용 log 파일 분석
==================

https://www.secrepo.com/self.logs/  사이트에서 로그 파일을 구했다. 
2017년 1월 14일 3시부터 2017년 1월 15일 1시 59분까지의 기록이 있는 로그 파일이다.

![image](https://github.com/user-attachments/assets/3e2407cf-d7c0-46cc-ac43-12dd65c74dec)

확장자 바꿔주고 wsl에서 진행한다.

![image](https://github.com/user-attachments/assets/15c4f1eb-b588-48dd-838f-dcf5213f17fa)

먼저 cat accesslog.txt|grep -E "14/Jan/2017" > date.log 명령어를 사용한다.

1.  **`cat accesslog.txt`**:
    * `cat` 명령어는 파일의 내용을 화면에 출력하는 데 사용된다.
    * `accesslog.txt` 파일의 **모든 내용**을 읽어서 다음 명령어로 전달한다.

2.  **`|` (파이프)**:
    * 파이프는 앞선 명령어의 출력을 다음 명령어의 입력으로 연결하는 역할을 한다.

3.  **`grep -E "14/Jan/2017"`**:
    * `grep` 명령어는 주어진 패턴과 일치하는 줄을 파일에서 찾아 출력하는 데 사용된다.
    * `-E` 옵션은 `grep`에게 **확장 정규 표현식 (Extended Regular Expression)**을 사용하도록 지시한다.
      이렇게 하면 더 복잡한 패턴 매칭이 가능해진다.
    * `"14/Jan/2017"`은 찾고자 하는 **패턴** 이다.
      따라서 `grep`은 입력으로 들어온 각 줄에서 `"14/Jan/2017"`이라는 문자열이 포함된 줄을 찾는다.

4.  **`>` (리디렉션)**:
    * 리디렉션은 명령어의 출력을 화면 대신 파일로 저장하는 데 사용된다.
    * `>` 기호는 앞선 명령어의 출력을 지정된 파일로 **덮어쓰기** 한다.
      만약 `date.log` 파일이 이미 존재한다면 그 내용은 지워지고 새로운 결과로 채워진다.
    * `date.log`는 `grep` 명령어의 결과 (즉, `"14/Jan/2017"`을 포함하는 줄들)가 저장될 **새로운 파일 이름** 이다.


결론적으로 이 명령어는 **`accesslog.txt` 파일의 모든 내용을 읽어서, 
그중에서 `"14/Jan/2017"`이라는 문자열을 포함하는 모든 줄을 찾아내어(즉 2017년 1월 14일에 작성된 모든 로그를 찾아내어),
그 결과를 `date.log`라는 새로운 파일에 저장한다.**

![image](https://github.com/user-attachments/assets/bccf3c93-bf60-419c-81a8-046c8e85e112)

data.log 파일을 열어보면 이렇게 14일 23시 59분까지의 기록만 추출된 것을 확인할 수 있다.

![image](https://github.com/user-attachments/assets/c2bfa509-40fd-4bfb-9e49-c715326227bb)

![image](https://github.com/user-attachments/assets/323ec90e-6a18-4eaa-89e5-713059cdee55)

 cat accesslog.txt|grep -E "14/Jan/2017:1[0-2]" 명령어를 사용했다.
 변경된 부분을 분석하면

* **`"14/Jan/2017:"`**: 이 부분은 이전과 동일하게 "14/Jan/2017:"라는 문자열을 찾는다.
  이는 2017년 1월 14일의 날짜 정보를 나타낸다. 로그 파일의 시간 형식이 `[dd/mmm/yyyy:hh:mm:ss]`와 유사하다고 가정했을 때, 날짜 부분까지 정확히 일치하는 줄을 찾는는다.

* **`1`**: 이 부분은 콜론(`:`) 뒤에 바로 숫자 `1`이 오는 경우를 찾는다.
  로그 시간에서 10시부터 19시까지의 시간을 나타내는 첫 번째 자리 숫자이다.

* **`[0-2]`**: 이 부분은 대괄호 `[]` 안에 있는 문자들 중 **하나**와 일치하는 부분을 찾는다.
  `0-2`는 숫자 0, 1, 또는 2 중 하나를 의미한다. 따라서 이 부분은 시간의 두 번째 자리가 0, 1, 또는 2인 경우를 찾는다.

결과적으로, `grep -E "14/Jan/2017:1[0-2]"`는 `accesslog.txt` 파일에서 

* **`14/Jan/2017:`** 로 시작하고,
* 그 뒤에 **`10`**, **`11`**, 또는 **`12`** 가 오는 경우

를 찾게된다.

즉, 이 명령어는 `accesslog.txt` 파일에서 **2017년 1월 14일의 오전 10시, 11시, 12시**의 로그 기록만을 추출하게 된다.


![image](https://github.com/user-attachments/assets/7cb252b6-346c-48b8-80e7-c86e8d495004)

이번에는 cat accesslog.txt| awk '{print $1}' | sort | uniq -c | sort -nr 명령어이다.

1. **| awk '{print $1}':** 파이프를 통해 cat의 출력이 awk 명령어로 전달된다.
   * awk '{print $1}'는 입력으로 들어온 각 줄에 대해 첫 번째 필드 ($1) 만을 출력한다.
   * 일반적으로 웹 서버 로그에서 첫 번째 필드는 접근한 클라이언트의 IP 주소인 경우가 많다.
따라서 이 단계에서는 로그 파일에서 모든 IP 주소를 한 줄에 하나씩 추출한다.

2. **| sort:** awk의 출력이 sort 명령어로 전달된다.
   * sort 명령어는 입력으로 들어온 줄들을 알파벳 순서대로 정렬한다.
   여기서는 추출된 IP 주소들이 정렬된다. 같은 IP 주소들이 연속된 줄에 나타나게 된다.

3. **| uniq -c:** sort의 출력이 uniq -c 명령어로 전달된다.
   * uniq 명령어는 연속적으로 반복되는 줄을 하나로 합쳐서 출력한다.
   * -c 옵션은 각 고유한 줄이 몇 번 반복되었는지 횟수를 함께 출력한다. 따라서 이 단계에서는 각 IP 주소별 접근 횟수가 출력된다. 출력 형식은 [횟수] [IP 주소] 형태가 된다.

4.  **| sort -nr:** uniq -c의 출력이 마지막 sort 명령어로 전달된다.
   * sort 명령어는 다시 한번 입력 줄들을 정렬한다.
   * -n 옵션은 입력된 줄을 숫자 순서대로 정렬한다. 여기서는 IP 주소 앞에 붙은 접근 횟수를 기준으로 정렬한다.
   * -r 옵션은 정렬 순서를 **내림차순 (reverse)**으로 변경한다. 따라서 접근 횟수가 많은 IP 주소부터 먼저 출력된다.


결론적으로, 이 명령어는 **accesslog.txt 파일에서 각 줄의 첫 번째 필드 (IP 주소라고 가정)를 추출하여, 
추출된 IP 주소들을 정렬한 후, 각 IP 주소별 접근 횟수를 세고, 그 결과를 접근 횟수가 많은 순서대로 정렬하여 출력한다.**

이 명령어를 실행하면 accesslog.txt 파일에 나타난 **IP 주소와 그 접근 횟수를 보여주며, 가장 많은 트래픽을 발생시킨 IP 주소부터 확인**할 수 있다.

실제로 출력된 결과에서 가장 많은 트래픽을 발생시킨 IP 주소가 77.22.30.202 임을 알 수 있다.



![image](https://github.com/user-attachments/assets/08e7fa2b-7149-48e7-bc77-b286ad89e4ed)

이번에는 cat accesslog.txt| awk -F" " '{print $1"\t"$4}' | sort | uniq -c | sort -nr | more 명령어를 사용했다.


1.  **`| awk -F" " '{print $1"\t"$4}'`**:
    * 파이프를 통해 `cat`의 출력이 `awk` 명령어로 전달된다.
    * **`-F" "`**: 이 부분은 `awk`의 **필드 구분자 (Field Separator)**를 **공백 문자 (" ")** 로 설정한다.
      로그 파일의 각 줄이 공백을 기준으로 여러 개의 필드로 나뉘게 됨을 의미한다.
    * **`'{print $1"\t"$4}'`**는 각 줄에 대해 **첫 번째 필드 (`$1`)** 와 **네 번째 필드 (`$4`)** 를 추출하여 탭 (`\t`)으로 구분하여 출력한다.
      웹 서버 로그에서 첫 번째 필드는 접근한 클라이언트의 IP 주소일 가능성이 높다.
      네 번째 필드는 시간 정보를 포함할 가능성이 높다.
      따라서 이 단계에서는 각 로그 줄에서 IP 주소와 시간 정보를 추출한다.

3.  **`| sort`**:
    * `awk`의 출력이 `sort` 명령어로 전달된다.
    * `sort` 명령어는 입력으로 들어온 줄들을 **알파벳 순서대로 정렬**한다.
      여기서는 IP 주소와 시간 정보가 탭으로 구분된 줄들이 정렬된다. 같은 IP 주소와 시간 조합이 연속된 줄에 나타나게 된다.

4.  **`| uniq -c`**:
    * `sort`의 출력이 `uniq -c` 명령어로 전달된다.
    * `uniq` 명령어는 **연속적으로 반복되는 줄**을 하나로 합쳐서 출력하고,
      `-c` 옵션은 각 고유한 줄이 몇 번 반복되었는지 **횟수를 함께 출력**한다.
      따라서 이 단계에서는 각 IP 주소와 특정 시간 조합의 접근 횟수가 출력된다.
      출력 형식은 `[횟수] [IP 주소]\t[시간]` 형태가 된다.

5.  **`| sort -nr`**:
    * `uniq -c`의 출력이 마지막 `sort` 명령어로 전달된다.
    * `sort -n` 옵션은 입력된 줄을 **숫자 순서**대로 정렬한다. 여기서 정렬 기준은 각 줄의 맨 앞에 있는 접근 횟수이다.
    * `-r` 옵션은 정렬 순서를 **내림차순 (reverse)**으로 변경한다.
      따라서 접근 횟수가 많은 IP 주소-시간 조합부터 먼저 출력된다.

6.  **`| more`**:
    * 최종 결과를 화면에 페이지 단위로 끊어서 보여준다. 

결론적으로, 이 명령어는 **`accesslog.txt` 파일에서 각 줄의 첫 번째 필드 (IP 주소)와 
네 번째 필드 (시간 정보)를 추출하여, 
이 조합을 기준으로 정렬한 후, 각 IP 주소-시간 조합별 접근 횟수를 세고, 
그 결과를 접근 횟수가 많은 순서대로 정렬하여 화면에 페이지 단위로 보여준다.**"



![image](https://github.com/user-attachments/assets/1d2b4441-e624-4f8e-affd-69b5fa1ff596)

이번에는 cat accesslog.txt| awk -F" " '{print $7}' | sort | uniq -c | sort -nr 명령어를 사용했다.

1.  **`| awk -F" " '{print $7}'`**:
    * 파이프를 통해 `cat`의 출력이 `awk` 명령어로 전달된다.
    * **`-F" "`**: `awk`의 **필드 구분자**를 **공백 문자 (" ")**로 설정한다.
    * **`'{print $7}'`**: 각 줄에 대해 **일곱 번째 필드 (`$7`)** 만을 추출하여 출력한다.
      이 일곱 번째 필드는 **요청된 리소스 즉 URL 경로 등** 일 가능성이 높다.

2.  **`| sort`**:
    * `awk`의 출력이 `sort` 명령어로 전달된다.
    * `sort` 명령어는 입력으로 들어온 줄들 (여기서는 일곱 번째 필드의 값들)을 **알파벳 순서대로 정렬**하여
      같은 값들이 연속된 줄에 나타나게 된다.

3.  **`| uniq -c`**:
    * `sort`의 출력이 `uniq -c` 명령어로 전달된다.
    * `uniq` 명령어는 **연속적으로 반복되는 줄**을 하나로 합쳐서 출력하고,
      `-c` 옵션은 각 고유한 줄이 몇 번 반복되었는지 **횟수를 함께 출력**한다.
      따라서 이 단계에서는 각 고유한 일곱 번째 필드 값의 접근 횟수가 출력된다.
      출력 형식은 `[횟수] [일곱 번째 필드 값]` 형태가 된다.

4.  **`| sort -nr`**:
    * `uniq -c`의 출력이 마지막 `sort` 명령어로 전달된다.
    * `sort -n` 옵션은 입력된 줄을 **숫자 순서**대로 정렬한다.
      여기서 정렬 기준은 각 줄의 맨 앞에 있는 접근 횟수이다.
    * `-r` 옵션은 정렬 순서를 **내림차순 (reverse)**으로 변경한다.
      따라서 가장 많이 접근된 일곱 번째 필드 값부터 먼저 출력된된다.

결론적으로, 이 명령어는 **`accesslog.txt` 파일에서 각 줄의 일곱 번째 필드의 값을 추출하여, 
이 값들을 정렬한 후, 각 고유한 값의 등장 횟수를 세고, 그 결과를 등장 횟수가 많은 순서대로 정렬하여 출력한다.**"

일곱 번째 필드의 값이 리소스(경로)이므로, 결국 경로별 count를 세는 명령어이다.


![image](https://github.com/user-attachments/assets/bcc12502-c01a-4dad-8cf7-bdb723051314)

이번에는 cat accesslog.txt | awk '{split($4, time, /[:\/]/); print su
bstr(time[4], 1, 2)":"substr(time[5], 1, 2), $1}' | sort | uniq -c | sort -nr | head -n 10 명령어를 사용했다.


1.  **`| awk '{split($4, time, /[:\/]/); print substr(time[4], 1, 2)":"substr(time[5], 1, 2), $1}'`**:
    * **`split($4, time, /[:\/]/)`**: 시간 정보를 필드별로 분리하는 것은 앞의 명령어들과 동일하다.
    * **`substr(time[4], 1, 2)":"substr(time[5], 1, 2)`**:
        * `time[4]`는 시간 (hour) 정보를 담고 있다.
          `substr(time[4], 1, 2)`는 시간 문자열의 첫 번째 글자부터 두 번째 글자까지 추출한다. (예: "21"에서 "21" 추출)
        * `time[5]`는 분 (minute) 정보를 담고 있다.
          `substr(time[5], 1, 2)`는 분 문자열의 첫 번째 글자부터 두 번째 글자까지 추출한다. (예: "30"에서 "30" 추출)
        * 이 둘을 콜론 (`:`)으로 연결하여 "HH:MM" 형식의 시간 정보를 만든다.
    * **`, $1`**: 추출된 "HH:MM" 형식의 시간과 IP 주소 (`$1`)를 공백으로 구분하여 출력한다.

3.  **`| sort`**:
    * `awk`의 출력 ("HH:MM IP_주소")를 시간 순으로 먼저 정렬하고, 같은 시간 내에서는 IP 주소 순으로 정렬한다.

4.  **`| uniq -c`**:
    * 정렬된 결과를 바탕으로 **연속적으로 동일한 "HH:MM IP_주소" 조합**의 빈도수를 계산하여 출력한다.
      결과는 `[횟수] HH:MM IP주소` 형태가 된다.

5.  **`| sort -nr`**:
    * 빈도수를 기준으로 내림차순 정렬한다.
      가장 많은 접근 횟수를 보인 "HH:MM IP주소" 조합이 먼저 나타난다.

6.  **`| head -n 10`**:
    * 가장 많은 상위 10개의 "HH:MM IP주소" 조합을 보여준다.


이 명령어는 **각 시간대 (시:분)별로 가장 많은 요청을 보낸 상위 10개의 IP 주소와 그 접근 횟수를 내림차순으로** 보여준다. 짧은 기간의 로그 파일에서 특정 시간대에 트래픽이 집중되는 IP 주소를 파악하거나, 비정상적인 활동을 보이는 IP 주소를 시간대별로 분석하는 데 유용할 수 있습니다.

예를 들어, 위에서 결과를 확인해보면 03:38에 77.22.30.202라는 IP 주소로부터 많은 요청(13회)이 발생했다는 것을 알 수 있다.
